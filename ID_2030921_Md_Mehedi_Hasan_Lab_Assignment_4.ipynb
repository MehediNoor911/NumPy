{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MehediNoor911/NumPy/blob/main/ID_2030921_Md_Mehedi_Hasan_Lab_Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWxyXBa8znbt"
      },
      "source": [
        "#### **Wheat Seed Classification**\n",
        "\n",
        "In this assignment, you will use the [Wheat Seed Dataset](https://archive.ics.uci.edu/ml/datasets/seeds) to classify the type of wheat seed based on the measurements of the seed. The dataset contains 7 attributes and 210 instances. The attributes are:\n",
        "\n",
        "1. Area\n",
        "2. Perimeter\n",
        "3. Compactness\n",
        "4. Length of Kernel\n",
        "5. Width of Kernel\n",
        "6. Asymmetry Coefficient\n",
        "7. Length of Kernel Groove\n",
        "\n",
        "Based on the attributes, the dataset contains 3 classes:\n",
        "\n",
        "1. Kama\n",
        "2. Rosa\n",
        "3. Canadian\n",
        "\n",
        "The text file `seeds_dataset.txt` contains the dataset. The first 7 columns are the attributes and the last column is the class label. The class labels are encoded as  1, 2, and 3 for Kama, Rosa, and Canadian, respectively. The goal of this assignment is to build a classifier that can predict the type of wheat seed based on the measurements of the seed. Follow the instructions below to complete the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAq8IStpznbu"
      },
      "source": [
        "\n",
        "#### **Step 1:** Download the dataset from [here](https://drive.google.com/file/d/1ZnGOVGFrNv0L1ctT8SO8Y3WfjD2HShgK/view?usp=sharing). It should be saved as `seeds_dataset.csv`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 2:** Upload the dataset to your Google Drive and mount your Google Drive to Colab.\n"
      ],
      "metadata": {
        "id": "mMMaH9XZq6Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "U_L-6evWCL6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code here\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wrj5rEhkqsmX",
        "outputId": "598af275-0b4b-4b73-f110-a8e3d2c33e27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 3:** Read the dataset using pandas' built-in function `pd.read_csv()` as `data` convert it into numpy array using `data.to_numpy()` function. Pass the following parameters to the `pd.read_csv()`function:\n",
        "    \n",
        "\n",
        "*   `filepath_or_buffer`: The path to the dataset\n",
        "*   `delimiter`: The delimiter used in the dataset to separate the attributes (Hint: Use `'\\t'` as the delimiter)\n",
        "*   `header`: The column header used in the dataset (Hint: Use `None` as the header)\n"
      ],
      "metadata": {
        "id": "QzgDpOJ_q2FO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/data"
      ],
      "metadata": {
        "id": "JMUO9lrrErff",
        "outputId": "0171f992-cc09-4cfe-88b8-29c3b484991e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "id": "syZ2AVZnEn4w",
        "outputId": "7c341480-bf83-4cf7-b8ab-a8a75f37291d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " bookprice.csv\t Dog\t\t\t       'Lenna_(test_image).png'   rainfall_data.csv\n",
            " Cat\t\t'gray_Lenna_(test_image).png'   PetImagesSmall.zip\t  seeds_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code here\n",
        "filepath_or_buffer = \"/content/drive/MyDrive/data/seeds_dataset.csv\"\n",
        "\n",
        "data = pd.read_csv(filepath_or_buffer, delimiter='\\t', header=None)\n",
        "\n",
        "data_in_array = data.to_numpy()\n",
        "\n",
        "print(data_in_array)\n"
      ],
      "metadata": {
        "id": "e3P-LnslbOzU",
        "outputId": "5f507aac-10ae-4e01-9dca-c546aa3d937d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[15.26   14.84    0.871  ...  2.221   5.22    1.    ]\n",
            " [14.88   14.57    0.8811 ...  1.018   4.956   1.    ]\n",
            " [14.29   14.09    0.905  ...  2.699   4.825   1.    ]\n",
            " ...\n",
            " [13.2    13.66    0.8883 ...  8.315   5.056   3.    ]\n",
            " [11.84   13.21    0.8521 ...  3.598   5.044   3.    ]\n",
            " [12.3    13.34    0.8684 ...  5.637   5.063   3.    ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Step 4:** Shuffle the dataset using `np.random.shuffle()`. Pass the following parameters to the function:\n",
        "* `x`: The dataset\n"
      ],
      "metadata": {
        "id": "vNyVHevnrleZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code here\n",
        "np.random.shuffle(data_in_array)\n",
        "print(data_in_array)\n"
      ],
      "metadata": {
        "id": "cY7keX9Xrw3M",
        "outputId": "a4c7f81d-9760-4e7b-e8d4-2bfdf63c25c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[16.44   15.25    0.888  ...  1.969   5.533   1.    ]\n",
            " [11.21   13.13    0.8167 ...  6.169   5.275   3.    ]\n",
            " [12.46   13.41    0.8706 ...  4.987   5.147   3.    ]\n",
            " ...\n",
            " [12.88   13.5     0.8879 ...  2.352   4.607   1.    ]\n",
            " [12.13   13.73    0.8081 ...  4.825   5.22    3.    ]\n",
            " [18.88   16.26    0.8969 ...  1.649   6.109   2.    ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 5:**  Split the dataset into features and labels. The first 7 columns of the dataset are the features and the last column is the label. Use numpy's array slicing to split the dataset into features and labels. (Hint: Use `:` to select all the rows and `0:7` to select the first 7 columns for features and `7` to select the last column for labels)\n"
      ],
      "metadata": {
        "id": "Zec6rLbDryO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code here\n",
        "\n",
        "features = data_in_array[:, :7]\n",
        "\n",
        "labels = data_in_array[:, 7]\n",
        "\n",
        "print(\"Features shape:\", features.shape)\n",
        "print(\"Labels shape:\", labels.shape)\n",
        "print(features)\n",
        "print(labels)\n"
      ],
      "metadata": {
        "id": "brUfzEJBr7Lm",
        "outputId": "1a9e7234-afe4-4b06-c681-880e5e297159",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features shape: (210, 7)\n",
            "Labels shape: (210,)\n",
            "[[15.26   14.84    0.871  ...  3.312   2.221   5.22  ]\n",
            " [14.88   14.57    0.8811 ...  3.333   1.018   4.956 ]\n",
            " [14.29   14.09    0.905  ...  3.337   2.699   4.825 ]\n",
            " ...\n",
            " [13.2    13.66    0.8883 ...  3.232   8.315   5.056 ]\n",
            " [11.84   13.21    0.8521 ...  2.836   3.598   5.044 ]\n",
            " [12.3    13.34    0.8684 ...  2.974   5.637   5.063 ]]\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 3. 3. 3. 3.\n",
            " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
            " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
            " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Another way\n",
        "# columns = data_in_array.shape[1]\n",
        "# features = data_in_array[:, :(columns-1)]\n",
        "\n",
        "# labels = data_in_array[:, (columns-1)]\n",
        "\n",
        "# print(\"Features shape:\", features.shape)\n",
        "# print(\"Labels shape:\", labels.shape)\n",
        "# print(features)\n",
        "# print(labels)"
      ],
      "metadata": {
        "id": "0vAhqgwtwOgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 6:**  Split the dataset into training and testing sets. Use numpy's built-in function `np.split()` to split the dataset into training and testing sets. Pass the following parameters to the function:\n",
        "* `ary`: The dataset\n",
        "* `indices_or_sections`: The number of instances in the training set (Hint: Use `int(0.8 * len(dataset))` to get the number of instances in the training set)\n",
        "* `axis`: The axis to split the dataset (Hint: Use `0` to split the dataset along the rows)\n"
      ],
      "metadata": {
        "id": "SUVjOZxxr9Ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code here\n",
        "train_size = int(0.8 * len(data_in_array))\n",
        "\n",
        "train_set, test_set = np.split(data_in_array, [train_size], axis=0)\n",
        "\n",
        "print(\"Training set shape:\", train_set.shape)\n",
        "print(\"Testing set shape:\", test_set.shape)\n",
        "\n",
        "#print(data_in_array[2, :])"
      ],
      "metadata": {
        "id": "P6uV-duTsRkW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be0933b5-a7a1-4a7e-f674-2977ed61d3d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (168, 8)\n",
            "Testing set shape: (42, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 7:**  Find the minimum and maximum values of each feature in the training set. Use numpy's built-in function `np.min()` and `np.max()` to find the minimum and maximum values of each feature in the training set. Pass the following parameters to the function:\n",
        "* `a`: The training set\n",
        "* `axis`: The axis to find the minimum and maximum values (Hint: Use `0` to find the minimum and maximum values along the columns)"
      ],
      "metadata": {
        "id": "zq0cHk5DsSTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code here\n",
        "min_values = np.min(train_set, axis=0)\n",
        "max_values = np.max(train_set, axis=0)\n",
        "# print(min_values)\n",
        "# print(max_values)"
      ],
      "metadata": {
        "id": "xwB4BJxHscLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Step 8:**  In this step, you must normalize the training and test sets. Nomalization is an essential part of every machine learning project. It is used to bring all the features to the same scale. If the features are not normalized, the higher-valued features will outnumber the lower-valued ones.\n",
        "\n",
        "For example, suppose we have a dataset with two features: the number of bedrooms in a house and the size of the garden in square feet and we are trying to forecast the rent of the residence. If the features are not normalized, the feature with higher values will take precedence over the feature with lower values. In this scenario, the garden area has a greater value. As a result, the model will make an attempt to forecast the house's price depending on the size of the garden. As a consequence, the model will be faulty since most individuals will not pay higher rent for more garden area. We need to normalize the features in order to prevent this. Let's look at the following illustration to better comprehend what we have said:\n",
        "* House 1: 2 bedrooms, 2500 sq. ft. garden\n",
        "* House 2: 3 bedrooms, 500 sq. ft. garden\n",
        "* House 3: 7 bedrooms, 2300 sq. ft. garden\n",
        "\n",
        "Considering that most people won't pay more for a larger garden, it follows that the rent for House 1 should be more comparable to House 2 than to House 3. However, if we give the aforementioned data to a k-NN classifier without normalization, it will compute the euclidean distance between the test and training examples and pick the class of the test instance based on the class of the closest training instance.\n",
        "\n",
        "The euclidean distance between the test instance and the training instances will be:\n",
        "\n",
        "* Distance between house 1 and house 2: $\\sqrt{(2-3)^2 + (2500-500)^2} = 2000$\n",
        "* Distance between house 1 and house 3: $\\sqrt{(2-7)^2 + (2500-2300)^2} = 200$\n",
        "\n",
        "As you can see, the distance between houses 1 and 3 is shorter than that between houses 1 and 2. As a result, the model will forecast that house 1 will cost around the same as house 3. This is not what was anticipated. We need to normalize the features in order to prevent this. To normalize the features, subtract the minimum value of each feature from all the values of that feature and divide the result by the range of the feature. The range of a feature is the difference between the maximum and minimum values of that feature. The formula for normalization is given below:\n",
        "\n",
        "$$x_{normalized} = \\frac{x - min(x)}{max(x) - min(x)}$$\n",
        "\n",
        "<html>\n",
        "<center>\n",
        "where, $x$ is the feature vector. The above formula will normalize the features to a scale of 0 to 1.\n",
        "</center>\n",
        "</html>\n",
        "\n",
        "\n",
        "\n",
        "Let's normalize the features in the above example. To do so, we need to find the minimum and maximum values of each feature. The minimum and maximum values of the number of bedrooms are 2 and 7, respectively. The minimum and maximum values of the garden area are 500 and 2500, respectively. The normalized values of the features are given below:\n",
        "\n",
        "* House 1: $(2 - 2) / 5 = 0$ bedrooms, $(2500 - 500) / 2000 = 0.75$ sq. ft. garden\n",
        "* House 2: $(3 - 2) / 5 = 0.2$ bedrooms, $(500 - 500) / 2000 = 0$ sq. ft. garden\n",
        "* House 3: $(7 - 2) / 5 = 1$ bedrooms, $(2300 - 500) / 2000 = 0.85$ sq. ft. garden\n",
        "\n",
        "Now, the euclidean distance between the test instance and the training instances will be:\n",
        "\n",
        "* Distance between house 1 and house 2: $\\sqrt{(0-0.2)^2 + (0.75-0)^2} = 0.77$\n",
        "* Distance between house 1 and house 3: $\\sqrt{(0-1)^2 + (0.75-0.9)^2} = 1.11$\n",
        "\n",
        "As you can see now, the distance between houses 1 and 2 is shorter than that between houses 1 and 3. The model will thus forecast that house 1 will cost about the same as house 2, according to the prediction. This is what is anticipated. This is what normalization does. It equalizes the scale of all features. This is important because it prevents the features with higher values from dominating the features with lower values.\n",
        "\n",
        "Use the minimum and maximum values you found in the previous step to normalize the training and test sets.\n"
      ],
      "metadata": {
        "id": "KTdEBT_Jsc9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code here\n",
        "train_data_normalized = (train_set - min_values) / (max_values - min_values)\n",
        "test_data_normalized = (test_set - min_values) / (max_values - min_values)\n",
        "\n",
        "print(\"Normalized Training Set:\")\n",
        "print(train_data_normalized)\n",
        "print(\"\\nNormalized Test Set:\")\n",
        "print(test_data_normalized)\n",
        "# print(train_set.shape)\n",
        "# print(train_set)\n",
        "# print(train_set - min_values)"
      ],
      "metadata": {
        "id": "d8NmhrprtWy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Step 9:**  Now, you have to build a classifier to classify the type of wheat seed based on the measurements of the seed. Use the K-Nearest Neighbors algorithm to build the classifier. Use the Euclidean distance to find the nearest neighbors.\n"
      ],
      "metadata": {
        "id": "3y_RpB0YtXxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code here\n",
        "def euclidean_distance(test_data_normalized, train_data_normalized):\n",
        "  return np.sqrt((test_data_normalized-train_data_normalized)**2)"
      ],
      "metadata": {
        "id": "sqvuOZ9Utdpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_nearest_neighbors(test_data_normalized, train_data_normalized, k=1):\n",
        "  distances = np.average(euclidean_distance(test_data_normalized, train_data_normalized), axis = 1)\n",
        "  sorted_indices = np.argsort(distances)\n",
        "  return train_set[sorted_indices[:k]]"
      ],
      "metadata": {
        "id": "RNYndCPHBONR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_k_closest_dataset = k_nearest_neighbors(test_data_normalized[4], train_data_normalized, k=5)\n",
        "print(\"First k numbers of closest dataset:\\n\")\n",
        "print(first_k_closest_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVn88Y8IEjc5",
        "outputId": "f2e3f50f-65f1-4c37-ee2e-25d23ef15fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First k numbers of closest dataset:\n",
            "\n",
            "[[11.36   13.05    0.8382  5.175   2.755   4.048   5.263   3.    ]\n",
            " [11.34   12.87    0.8596  5.053   2.849   3.347   5.003   3.    ]\n",
            " [11.18   13.04    0.8266  5.22    2.693   3.332   5.001   3.    ]\n",
            " [12.05   13.41    0.8416  5.267   2.847   4.988   5.046   3.    ]\n",
            " [11.82   13.4     0.8274  5.314   2.777   4.471   5.178   3.    ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Step 10:**  Output the number of data points in the testing set and the number of correct predictions made by the classifier for each class."
      ],
      "metadata": {
        "id": "cCXjvR3AteaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code here\n",
        "def predict(test_data_normalized, train_data_normalized, k=1):\n",
        "    predicted_dataset = np.empty(len(test_data_normalized))\n",
        "    for i, test_data_normalized in enumerate(test_data_normalized):\n",
        "        predicted_dataset[i] = k_nearest_neighbors(test_data_normalized, train_data_normalized, k)\n",
        "    return predicted_dataset\n"
      ],
      "metadata": {
        "id": "CLR_LFwvti0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(test_data_normalized.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N34ACVHrcH0Z",
        "outputId": "28e58931-8e50-47f1-d2f0-9ba4da00f4a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(test_data_normalized.shape[0]):\n",
        "    predicted_dataset = predict(test_data_normalized[i], train_data_normalized, 1)\n",
        "    print(\"Actual data:\", test_data_normalized[i], \"predicted_dataset:\", predicted_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm8W9Hj8UIcW",
        "outputId": "ecc6b4ba-d813-4aff-f0c0-c0bbfce8f9ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual data: [0.05389798 0.10606061 0.19056261 0.15454033 0.01444043 0.56969011\n",
            " 0.30182176 1.        ] predicted_dataset: [[11.36   13.05    0.8382  5.175   2.755   4.048   5.263   3.    ]]\n",
            "Actual data: [0.04331088 0.08008658 0.2522686  0.10603497 0.04837545 0.43954449\n",
            " 0.28015756 1.        ] predicted_dataset: [[11.18   13.04    0.8266  5.22    2.693   3.332   5.001   3.    ]]\n",
            "Actual data: [0.02213667 0.08008658 0.09800363 0.23857868 0.03826715 0.95215235\n",
            " 0.31708518 1.        ] predicted_dataset: [[11.21   13.13    0.8167  5.279   2.687   6.169   5.275   3.    ]]\n",
            "Actual data: [0.07314726 0.1017316  0.33938294 0.14946418 0.14223827 0.9489625\n",
            " 0.21516494 1.        ] predicted_dataset: [[11.14   12.79    0.8558  5.011   2.794   6.388   5.049   3.    ]]\n",
            "Actual data: [0.04619827 0.07359307 0.30671506 0.10490694 0.08303249 0.56522433\n",
            " 0.23682915 1.        ] predicted_dataset: [[11.36   13.05    0.8382  5.175   2.755   4.048   5.263   3.    ]]\n",
            "Actual data: [0.0587103  0.0974026  0.26678766 0.1319797  0.08303249 0.76921482\n",
            " 0.28064993 1.        ] predicted_dataset: [[11.19   13.05    0.8253  5.25    2.675   5.813   5.219   3.    ]]\n",
            "Actual data: [ 0.00384986  0.07142857  0.01633394  0.21206994 -0.00505415  0.704461\n",
            "  0.32791728  1.        ] predicted_dataset: [[10.79   12.93    0.8107  5.317   2.648   5.462   5.194   3.    ]]\n",
            "Actual data: [ 9.62463908e-04 -1.29870130e-02  4.61887477e-01  4.45572476e-02\n",
            "  1.24909747e-01  6.39228696e-01  2.67848351e-01  1.00000000e+00] predicted_dataset: [[11.14   12.79    0.8558  5.011   2.794   6.388   5.049   3.    ]]\n",
            "Actual data: [0.0452358  0.08225108 0.24863884 0.16018049 0.04476534 0.7288633\n",
            " 0.28212703 1.        ] predicted_dataset: [[11.19   13.05    0.8253  5.25    2.675   5.813   5.219   3.    ]]\n",
            "Actual data: [-0.00481232  0.02164502  0.22504537  0.13705584 -0.00433213  0.62790475\n",
            "  0.21861152  1.        ] predicted_dataset: [[11.18   13.04    0.8266  5.22    2.693   3.332   5.001   3.    ]]\n",
            "Actual data: [0.06641001 0.09090909 0.35571688 0.15679639 0.07942238 0.81514857\n",
            " 0.23781388 1.        ] predicted_dataset: [[11.14   12.79    0.8558  5.011   2.794   6.388   5.049   3.    ]]\n",
            "Actual data: [0.13666987 0.18181818 0.33756806 0.25662719 0.17689531 0.14288904\n",
            " 0.3244707  1.        ] predicted_dataset: [[12.7    13.71    0.8491  5.386   2.911   3.26    5.316   3.    ]]\n",
            "Actual data: [0.05967276 0.06926407 0.43466425 0.10603497 0.09169675 0.66857526\n",
            " 0.1506647  1.        ] predicted_dataset: [[11.14   12.79    0.8558  5.011   2.794   6.388   5.049   3.    ]]\n",
            "Actual data: [0.16073147 0.16883117 0.56715064 0.18838127 0.26642599 0.67336002\n",
            " 0.30920729 1.        ] predicted_dataset: [[12.22   13.32    0.8652  5.224   2.967   5.469   5.221   3.    ]]\n",
            "Actual data: [0.13474495 0.15800866 0.45190563 0.19063734 0.18844765 0.65262604\n",
            " 0.31462334 1.        ] predicted_dataset: [[12.22   13.32    0.8652  5.224   2.967   5.469   5.221   3.    ]]\n",
            "Actual data: [0.0827719  0.0952381  0.44827586 0.11618725 0.14584838 0.70876728\n",
            " 0.30329887 1.        ] predicted_dataset: [[11.83   13.23    0.8496  5.263   2.84    5.195   5.307   3.    ]]\n",
            "Actual data: [0.20211742 0.24675325 0.41742287 0.33446136 0.27292419 0.86443165\n",
            " 0.39241753 1.        ] predicted_dataset: [[13.34  13.95   0.862  5.389  3.074  5.995  5.307  3.   ]]\n",
            "Actual data: [0.07410972 0.14718615 0.1061706  0.26001128 0.02527076 0.52582976\n",
            " 0.32644018 1.        ] predicted_dataset: [[11.82   13.4     0.8274  5.314   2.777   4.471   5.178   3.    ]]\n",
            "Actual data: [0.09817132 0.17748918 0.1061706  0.28821207 0.04909747 0.65916522\n",
            " 0.41014279 1.        ] predicted_dataset: [[11.75   13.52    0.8082  5.444   2.678   4.378   5.31    3.    ]]\n",
            "Actual data: [0.01154957 0.03679654 0.26406534 0.10490694 0.01949458 0.54449034\n",
            " 0.21516494 1.        ] predicted_dataset: [[11.36   13.05    0.8382  5.175   2.755   4.048   5.263   3.    ]]\n",
            "Actual data: [0.04234841 0.04112554 0.46551724 0.10547095 0.12490975 1.07799167\n",
            " 0.21565731 1.        ] predicted_dataset: [[11.14   12.79    0.8558  5.011   2.794   6.388   5.049   3.    ]]\n",
            "Actual data: [-0.01924928 -0.04761905  0.51451906 -0.00169205  0.10036101  0.67144612\n",
            "  0.13540128  1.        ] predicted_dataset: [[11.14   12.79    0.8558  5.011   2.794   6.388   5.049   3.    ]]\n",
            "Actual data: [0.01347449 0.03679654 0.28039927 0.08121827 0.04981949 0.73891131\n",
            " 0.25898572 1.        ] predicted_dataset: [[11.14   12.79    0.8558  5.011   2.794   6.388   5.049   3.    ]]\n",
            "Actual data: [0.04619827 0.04978355 0.43738657 0.10659898 0.11263538 0.51354886\n",
            " 0.23732152 1.        ] predicted_dataset: [[11.34   12.87    0.8596  5.053   2.849   3.347   5.003   3.    ]]\n",
            "Actual data: [0.1039461  0.08441558 0.64791289 0.12972363 0.22021661 0.4516659\n",
            " 0.30182176 1.        ] predicted_dataset: [[12.1    13.15    0.8793  5.105   2.941   2.201   5.056   3.    ]]\n",
            "Actual data: [ 0.00288739  0.04329004  0.15880218  0.15679639 -0.01299639  0.65198807\n",
            "  0.28064993  1.        ] predicted_dataset: [[11.19   13.05    0.8253  5.25    2.675   5.813   5.219   3.    ]]\n",
            "Actual data: [0.12704524 0.13852814 0.50635209 0.18838127 0.23610108 0.53699421\n",
            " 0.24273757 1.        ] predicted_dataset: [[12.49   13.46    0.8658  5.267   2.967   4.421   5.002   3.    ]]\n",
            "Actual data: [0.19345525 0.18181818 0.70689655 0.14551607 0.34512635 0.65517791\n",
            " 0.19448548 1.        ] predicted_dataset: [[12.49   13.46    0.8658  5.267   2.967   4.421   5.002   3.    ]]\n",
            "Actual data: [0.19249278 0.19480519 0.63974592 0.18161309 0.29314079 0.75246814\n",
            " 0.21614968 1.        ] predicted_dataset: [[12.22   13.32    0.8652  5.224   2.967   5.469   5.221   3.    ]]\n",
            "Actual data: [0.24831569 0.24891775 0.6969147  0.2357586  0.3465704  0.622801\n",
            " 0.28163466 1.        ] predicted_dataset: [[12.49   13.46    0.8658  5.267   2.967   4.421   5.002   3.    ]]\n",
            "Actual data: [0.1761309  0.22510823 0.36297641 0.28652002 0.1898917  0.40525367\n",
            " 0.35056622 1.        ] predicted_dataset: [[12.7    13.71    0.8491  5.386   2.911   3.26    5.316   3.    ]]\n",
            "Actual data: [0.18960539 0.16233766 0.80127042 0.0964467  0.36606498 0.32901641\n",
            " 0.15312654 1.        ] predicted_dataset: [[12.1    13.15    0.8793  5.105   2.941   2.201   5.056   3.    ]]\n",
            "Actual data: [0.15303176 0.17532468 0.47912886 0.17879301 0.24620939 0.75071373\n",
            " 0.25898572 1.        ] predicted_dataset: [[12.22   13.32    0.8652  5.224   2.967   5.469   5.221   3.    ]]\n",
            "Actual data: [0.18094321 0.14935065 0.81306715 0.04624929 0.35162455 0.24480454\n",
            " 0.11127523 1.        ] predicted_dataset: [[12.1    13.15    0.8793  5.105   2.941   2.201   5.056   3.    ]]\n",
            "Actual data: [0.03753609 0.01948052 0.54355717 0.06034969 0.11696751 0.52407534\n",
            " 0.1521418  1.        ] predicted_dataset: [[11.34   12.87    0.8596  5.053   2.849   3.347   5.003   3.    ]]\n",
            "Actual data: [0.18383061 0.16883117 0.71960073 0.15848844 0.3198556  1.22663838\n",
            " 0.23682915 1.        ] predicted_dataset: [[12.22   13.32    0.8652  5.224   2.967   5.469   5.221   3.    ]]\n",
            "Actual data: [0.1520693  0.18181818 0.44101633 0.17033277 0.22527076 0.50302238\n",
            " 0.23732152 1.        ] predicted_dataset: [[12.49   13.46    0.8658  5.267   2.967   4.421   5.002   3.    ]]\n",
            "Actual data: [0.13474495 0.12337662 0.63702359 0.13254371 0.24043321 0.45708863\n",
            " 0.17282127 1.        ] predicted_dataset: [[12.1    13.15    0.8793  5.105   2.941   2.201   5.056   3.    ]]\n",
            "Actual data: [0.04234841 0.05411255 0.39019964 0.13423576 0.10613718 0.5677762\n",
            " 0.23830625 1.        ] predicted_dataset: [[11.34   12.87    0.8596  5.053   2.849   3.347   5.003   3.    ]]\n",
            "Actual data: [0.2319538  0.22294372 0.7277677  0.18838127 0.42166065 1.20414999\n",
            " 0.26440177 1.        ] predicted_dataset: [[13.32   13.94    0.8613  5.541   3.073   7.035   5.44    3.    ]]\n",
            "Actual data: [0.10105871 0.12554113 0.39927405 0.15397631 0.13574007 0.45182539\n",
            " 0.25849335 1.        ] predicted_dataset: [[11.34   12.87    0.8596  5.053   2.849   3.347   5.003   3.    ]]\n",
            "Actual data: [0.14533205 0.15367965 0.54718693 0.19232939 0.23537906 0.77702994\n",
            " 0.26784835 1.        ] predicted_dataset: [[12.22   13.32    0.8652  5.224   2.967   5.469   5.221   3.    ]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b148fc9bfa8b60132af830e32e1690e4e023b803e92912df15b823b90141dda6"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}